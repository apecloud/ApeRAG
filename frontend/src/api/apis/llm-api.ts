/* tslint:disable */
/* eslint-disable */
/**
 * ApeRAG API
 * ApeRAG API Documentation
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


import type { Configuration } from '../configuration';
import type { AxiosPromise, AxiosInstance, RawAxiosRequestConfig } from 'axios';
import globalAxios from 'axios';
// Some imports not used depending on template conditions
// @ts-ignore
import { DUMMY_BASE_URL, assertParamExists, setApiKeyToObject, setBasicAuthToObject, setBearerAuthToObject, setOAuthToObject, setSearchParams, serializeDataIfNeeded, toPathString, createRequestFunction } from '../common';
// @ts-ignore
import { BASE_PATH, COLLECTION_FORMATS, type RequestArgs, BaseAPI, RequiredError, operationServerMap } from '../base';
// @ts-ignore
import type { EmbeddingRequest } from '../models';
// @ts-ignore
import type { EmbeddingResponse } from '../models';
// @ts-ignore
import type { FailResponse } from '../models';
// @ts-ignore
import type { LlmConfigurationResponse } from '../models';
// @ts-ignore
import type { LlmProvider } from '../models';
// @ts-ignore
import type { LlmProviderCreateWithApiKey } from '../models';
// @ts-ignore
import type { LlmProviderModel } from '../models';
// @ts-ignore
import type { LlmProviderModelCreate } from '../models';
// @ts-ignore
import type { LlmProviderModelList } from '../models';
// @ts-ignore
import type { LlmProviderModelUpdate } from '../models';
// @ts-ignore
import type { LlmProviderUpdateWithApiKey } from '../models';
// @ts-ignore
import type { ModelConfigList } from '../models';
// @ts-ignore
import type { RerankRequest } from '../models';
// @ts-ignore
import type { RerankResponse } from '../models';
// @ts-ignore
import type { TagFilterRequest } from '../models';
/**
 * LlmApi - axios parameter creator
 * @export
 */
export const LlmApiAxiosParamCreator = function (configuration?: Configuration) {
    return {
        /**
         * Get available models with tag filtering support. - No tag_filters or empty array: Returns only models with \"recommend\" tag (default) - Specific filters: Returns models matching the filter conditions 
         * @summary Get available models with filtering
         * @param {TagFilterRequest} [tagFilterRequest] 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        availableModelsPost: async (tagFilterRequest?: TagFilterRequest, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            const localVarPath = `/available_models`;
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'POST', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;


    
            localVarHeaderParameter['Content-Type'] = 'application/json';

            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};
            localVarRequestOptions.data = serializeDataIfNeeded(tagFilterRequest, localVarRequestOptions, configuration)

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * Generate embeddings for the given input text(s) using the specified provider and model. This endpoint is compatible with OpenAI\'s embeddings API format, but includes an additional \'provider\' parameter to specify which LLM provider to use.  The endpoint supports both single text inputs and batch processing of multiple texts. It requires the provider to be configured in the user\'s Model Service Provider (MSP) settings with a valid API key. 
         * @summary Create embeddings
         * @param {EmbeddingRequest} embeddingRequest 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        embeddingsPost: async (embeddingRequest: EmbeddingRequest, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'embeddingRequest' is not null or undefined
            assertParamExists('embeddingsPost', 'embeddingRequest', embeddingRequest)
            const localVarPath = `/embeddings`;
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'POST', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;

            // authentication CookieAuth required

            // authentication BearerAuth required
            // http bearer authentication required
            await setBearerAuthToObject(localVarHeaderParameter, configuration)


    
            localVarHeaderParameter['Content-Type'] = 'application/json';

            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};
            localVarRequestOptions.data = serializeDataIfNeeded(embeddingRequest, localVarRequestOptions, configuration)

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * Get complete LLM configuration including providers and models
         * @summary Get complete LLM configuration
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmConfigurationGet: async (options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            const localVarPath = `/llm_configuration`;
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'GET', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;


    
            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * List all LLM provider models, optionally filtered by provider
         * @summary List all LLM provider models
         * @param {string} [providerName] Optional filter by provider name
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProviderModelsGet: async (providerName?: string, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            const localVarPath = `/llm_provider_models`;
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'GET', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;

            if (providerName !== undefined) {
                localVarQueryParameter['provider_name'] = providerName;
            }


    
            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * Create a new LLM provider with optional API key
         * @summary Create a new LLM provider
         * @param {LlmProviderCreateWithApiKey} llmProviderCreateWithApiKey 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersPost: async (llmProviderCreateWithApiKey: LlmProviderCreateWithApiKey, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'llmProviderCreateWithApiKey' is not null or undefined
            assertParamExists('llmProvidersPost', 'llmProviderCreateWithApiKey', llmProviderCreateWithApiKey)
            const localVarPath = `/llm_providers`;
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'POST', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;


    
            localVarHeaderParameter['Content-Type'] = 'application/json';

            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};
            localVarRequestOptions.data = serializeDataIfNeeded(llmProviderCreateWithApiKey, localVarRequestOptions, configuration)

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * Delete an LLM provider (soft delete)
         * @summary Delete an LLM provider
         * @param {string} providerName Provider name
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersProviderNameDelete: async (providerName: string, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'providerName' is not null or undefined
            assertParamExists('llmProvidersProviderNameDelete', 'providerName', providerName)
            const localVarPath = `/llm_providers/{provider_name}`
                .replace(`{${"provider_name"}}`, encodeURIComponent(String(providerName)));
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'DELETE', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;


    
            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * Get a specific LLM provider by name
         * @summary Get a specific LLM provider
         * @param {string} providerName Provider name
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersProviderNameGet: async (providerName: string, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'providerName' is not null or undefined
            assertParamExists('llmProvidersProviderNameGet', 'providerName', providerName)
            const localVarPath = `/llm_providers/{provider_name}`
                .replace(`{${"provider_name"}}`, encodeURIComponent(String(providerName)));
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'GET', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;


    
            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * Delete a specific model of a provider
         * @summary Delete a provider model
         * @param {string} providerName Provider name
         * @param {LlmProvidersProviderNameModelsApiModelDeleteApiEnum} api API type
         * @param {string} model Model name (supports names with slashes, will be URL decoded)
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersProviderNameModelsApiModelDelete: async (providerName: string, api: LlmProvidersProviderNameModelsApiModelDeleteApiEnum, model: string, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'providerName' is not null or undefined
            assertParamExists('llmProvidersProviderNameModelsApiModelDelete', 'providerName', providerName)
            // verify required parameter 'api' is not null or undefined
            assertParamExists('llmProvidersProviderNameModelsApiModelDelete', 'api', api)
            // verify required parameter 'model' is not null or undefined
            assertParamExists('llmProvidersProviderNameModelsApiModelDelete', 'model', model)
            const localVarPath = `/llm_providers/{provider_name}/models/{api}/{model}`
                .replace(`{${"provider_name"}}`, encodeURIComponent(String(providerName)))
                .replace(`{${"api"}}`, encodeURIComponent(String(api)))
                .replace(`{${"model"}}`, encodeURIComponent(String(model)));
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'DELETE', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;


    
            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * Update a specific model of a provider
         * @summary Update a provider model
         * @param {string} providerName Provider name
         * @param {LlmProvidersProviderNameModelsApiModelPutApiEnum} api API type
         * @param {string} model Model name (supports names with slashes, will be URL decoded)
         * @param {LlmProviderModelUpdate} llmProviderModelUpdate 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersProviderNameModelsApiModelPut: async (providerName: string, api: LlmProvidersProviderNameModelsApiModelPutApiEnum, model: string, llmProviderModelUpdate: LlmProviderModelUpdate, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'providerName' is not null or undefined
            assertParamExists('llmProvidersProviderNameModelsApiModelPut', 'providerName', providerName)
            // verify required parameter 'api' is not null or undefined
            assertParamExists('llmProvidersProviderNameModelsApiModelPut', 'api', api)
            // verify required parameter 'model' is not null or undefined
            assertParamExists('llmProvidersProviderNameModelsApiModelPut', 'model', model)
            // verify required parameter 'llmProviderModelUpdate' is not null or undefined
            assertParamExists('llmProvidersProviderNameModelsApiModelPut', 'llmProviderModelUpdate', llmProviderModelUpdate)
            const localVarPath = `/llm_providers/{provider_name}/models/{api}/{model}`
                .replace(`{${"provider_name"}}`, encodeURIComponent(String(providerName)))
                .replace(`{${"api"}}`, encodeURIComponent(String(api)))
                .replace(`{${"model"}}`, encodeURIComponent(String(model)));
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'PUT', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;


    
            localVarHeaderParameter['Content-Type'] = 'application/json';

            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};
            localVarRequestOptions.data = serializeDataIfNeeded(llmProviderModelUpdate, localVarRequestOptions, configuration)

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * Get all models for a specific provider
         * @summary Get models for a specific provider
         * @param {string} providerName Provider name
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersProviderNameModelsGet: async (providerName: string, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'providerName' is not null or undefined
            assertParamExists('llmProvidersProviderNameModelsGet', 'providerName', providerName)
            const localVarPath = `/llm_providers/{provider_name}/models`
                .replace(`{${"provider_name"}}`, encodeURIComponent(String(providerName)));
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'GET', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;


    
            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * Create a new model for a specific provider
         * @summary Create a new model for a provider
         * @param {string} providerName Provider name
         * @param {LlmProviderModelCreate} llmProviderModelCreate 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersProviderNameModelsPost: async (providerName: string, llmProviderModelCreate: LlmProviderModelCreate, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'providerName' is not null or undefined
            assertParamExists('llmProvidersProviderNameModelsPost', 'providerName', providerName)
            // verify required parameter 'llmProviderModelCreate' is not null or undefined
            assertParamExists('llmProvidersProviderNameModelsPost', 'llmProviderModelCreate', llmProviderModelCreate)
            const localVarPath = `/llm_providers/{provider_name}/models`
                .replace(`{${"provider_name"}}`, encodeURIComponent(String(providerName)));
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'POST', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;


    
            localVarHeaderParameter['Content-Type'] = 'application/json';

            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};
            localVarRequestOptions.data = serializeDataIfNeeded(llmProviderModelCreate, localVarRequestOptions, configuration)

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * Update an existing LLM provider with optional API key
         * @summary Update an LLM provider
         * @param {string} providerName Provider name
         * @param {LlmProviderUpdateWithApiKey} llmProviderUpdateWithApiKey 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersProviderNamePut: async (providerName: string, llmProviderUpdateWithApiKey: LlmProviderUpdateWithApiKey, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'providerName' is not null or undefined
            assertParamExists('llmProvidersProviderNamePut', 'providerName', providerName)
            // verify required parameter 'llmProviderUpdateWithApiKey' is not null or undefined
            assertParamExists('llmProvidersProviderNamePut', 'llmProviderUpdateWithApiKey', llmProviderUpdateWithApiKey)
            const localVarPath = `/llm_providers/{provider_name}`
                .replace(`{${"provider_name"}}`, encodeURIComponent(String(providerName)));
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'PUT', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;


    
            localVarHeaderParameter['Content-Type'] = 'application/json';

            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};
            localVarRequestOptions.data = serializeDataIfNeeded(llmProviderUpdateWithApiKey, localVarRequestOptions, configuration)

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * Rerank a list of documents based on their relevance to a given query using the specified  provider and model. This endpoint follows the industry-standard rerank API format used by providers like Cohere, Jina AI, and others.  The endpoint supports both simple text lists and structured document objects with metadata. Documents are returned ordered by relevance score (highest first), with optional top_k  filtering to limit the number of results.  The provider must be configured in the user\'s Model Service Provider (MSP) settings with a valid API key and support rerank functionality. 
         * @summary Rerank documents
         * @param {RerankRequest} rerankRequest 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        rerankPost: async (rerankRequest: RerankRequest, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'rerankRequest' is not null or undefined
            assertParamExists('rerankPost', 'rerankRequest', rerankRequest)
            const localVarPath = `/rerank`;
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'POST', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;

            // authentication CookieAuth required

            // authentication BearerAuth required
            // http bearer authentication required
            await setBearerAuthToObject(localVarHeaderParameter, configuration)


    
            localVarHeaderParameter['Content-Type'] = 'application/json';

            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};
            localVarRequestOptions.data = serializeDataIfNeeded(rerankRequest, localVarRequestOptions, configuration)

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
    }
};

/**
 * LlmApi - functional programming interface
 * @export
 */
export const LlmApiFp = function(configuration?: Configuration) {
    const localVarAxiosParamCreator = LlmApiAxiosParamCreator(configuration)
    return {
        /**
         * Get available models with tag filtering support. - No tag_filters or empty array: Returns only models with \"recommend\" tag (default) - Specific filters: Returns models matching the filter conditions 
         * @summary Get available models with filtering
         * @param {TagFilterRequest} [tagFilterRequest] 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async availableModelsPost(tagFilterRequest?: TagFilterRequest, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<ModelConfigList>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.availableModelsPost(tagFilterRequest, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['LlmApi.availableModelsPost']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
        /**
         * Generate embeddings for the given input text(s) using the specified provider and model. This endpoint is compatible with OpenAI\'s embeddings API format, but includes an additional \'provider\' parameter to specify which LLM provider to use.  The endpoint supports both single text inputs and batch processing of multiple texts. It requires the provider to be configured in the user\'s Model Service Provider (MSP) settings with a valid API key. 
         * @summary Create embeddings
         * @param {EmbeddingRequest} embeddingRequest 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async embeddingsPost(embeddingRequest: EmbeddingRequest, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<EmbeddingResponse>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.embeddingsPost(embeddingRequest, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['LlmApi.embeddingsPost']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
        /**
         * Get complete LLM configuration including providers and models
         * @summary Get complete LLM configuration
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async llmConfigurationGet(options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<LlmConfigurationResponse>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.llmConfigurationGet(options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['LlmApi.llmConfigurationGet']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
        /**
         * List all LLM provider models, optionally filtered by provider
         * @summary List all LLM provider models
         * @param {string} [providerName] Optional filter by provider name
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async llmProviderModelsGet(providerName?: string, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<LlmProviderModelList>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.llmProviderModelsGet(providerName, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['LlmApi.llmProviderModelsGet']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
        /**
         * Create a new LLM provider with optional API key
         * @summary Create a new LLM provider
         * @param {LlmProviderCreateWithApiKey} llmProviderCreateWithApiKey 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async llmProvidersPost(llmProviderCreateWithApiKey: LlmProviderCreateWithApiKey, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<LlmProvider>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.llmProvidersPost(llmProviderCreateWithApiKey, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['LlmApi.llmProvidersPost']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
        /**
         * Delete an LLM provider (soft delete)
         * @summary Delete an LLM provider
         * @param {string} providerName Provider name
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async llmProvidersProviderNameDelete(providerName: string, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<void>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.llmProvidersProviderNameDelete(providerName, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['LlmApi.llmProvidersProviderNameDelete']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
        /**
         * Get a specific LLM provider by name
         * @summary Get a specific LLM provider
         * @param {string} providerName Provider name
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async llmProvidersProviderNameGet(providerName: string, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<LlmProvider>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.llmProvidersProviderNameGet(providerName, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['LlmApi.llmProvidersProviderNameGet']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
        /**
         * Delete a specific model of a provider
         * @summary Delete a provider model
         * @param {string} providerName Provider name
         * @param {LlmProvidersProviderNameModelsApiModelDeleteApiEnum} api API type
         * @param {string} model Model name (supports names with slashes, will be URL decoded)
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async llmProvidersProviderNameModelsApiModelDelete(providerName: string, api: LlmProvidersProviderNameModelsApiModelDeleteApiEnum, model: string, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<void>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.llmProvidersProviderNameModelsApiModelDelete(providerName, api, model, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['LlmApi.llmProvidersProviderNameModelsApiModelDelete']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
        /**
         * Update a specific model of a provider
         * @summary Update a provider model
         * @param {string} providerName Provider name
         * @param {LlmProvidersProviderNameModelsApiModelPutApiEnum} api API type
         * @param {string} model Model name (supports names with slashes, will be URL decoded)
         * @param {LlmProviderModelUpdate} llmProviderModelUpdate 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async llmProvidersProviderNameModelsApiModelPut(providerName: string, api: LlmProvidersProviderNameModelsApiModelPutApiEnum, model: string, llmProviderModelUpdate: LlmProviderModelUpdate, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<LlmProviderModel>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.llmProvidersProviderNameModelsApiModelPut(providerName, api, model, llmProviderModelUpdate, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['LlmApi.llmProvidersProviderNameModelsApiModelPut']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
        /**
         * Get all models for a specific provider
         * @summary Get models for a specific provider
         * @param {string} providerName Provider name
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async llmProvidersProviderNameModelsGet(providerName: string, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<LlmProviderModelList>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.llmProvidersProviderNameModelsGet(providerName, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['LlmApi.llmProvidersProviderNameModelsGet']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
        /**
         * Create a new model for a specific provider
         * @summary Create a new model for a provider
         * @param {string} providerName Provider name
         * @param {LlmProviderModelCreate} llmProviderModelCreate 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async llmProvidersProviderNameModelsPost(providerName: string, llmProviderModelCreate: LlmProviderModelCreate, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<LlmProviderModel>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.llmProvidersProviderNameModelsPost(providerName, llmProviderModelCreate, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['LlmApi.llmProvidersProviderNameModelsPost']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
        /**
         * Update an existing LLM provider with optional API key
         * @summary Update an LLM provider
         * @param {string} providerName Provider name
         * @param {LlmProviderUpdateWithApiKey} llmProviderUpdateWithApiKey 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async llmProvidersProviderNamePut(providerName: string, llmProviderUpdateWithApiKey: LlmProviderUpdateWithApiKey, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<LlmProvider>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.llmProvidersProviderNamePut(providerName, llmProviderUpdateWithApiKey, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['LlmApi.llmProvidersProviderNamePut']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
        /**
         * Rerank a list of documents based on their relevance to a given query using the specified  provider and model. This endpoint follows the industry-standard rerank API format used by providers like Cohere, Jina AI, and others.  The endpoint supports both simple text lists and structured document objects with metadata. Documents are returned ordered by relevance score (highest first), with optional top_k  filtering to limit the number of results.  The provider must be configured in the user\'s Model Service Provider (MSP) settings with a valid API key and support rerank functionality. 
         * @summary Rerank documents
         * @param {RerankRequest} rerankRequest 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async rerankPost(rerankRequest: RerankRequest, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<RerankResponse>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.rerankPost(rerankRequest, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['LlmApi.rerankPost']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
    }
};

/**
 * LlmApi - factory interface
 * @export
 */
export const LlmApiFactory = function (configuration?: Configuration, basePath?: string, axios?: AxiosInstance) {
    const localVarFp = LlmApiFp(configuration)
    return {
        /**
         * Get available models with tag filtering support. - No tag_filters or empty array: Returns only models with \"recommend\" tag (default) - Specific filters: Returns models matching the filter conditions 
         * @summary Get available models with filtering
         * @param {LlmApiAvailableModelsPostRequest} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        availableModelsPost(requestParameters: LlmApiAvailableModelsPostRequest = {}, options?: RawAxiosRequestConfig): AxiosPromise<ModelConfigList> {
            return localVarFp.availableModelsPost(requestParameters.tagFilterRequest, options).then((request) => request(axios, basePath));
        },
        /**
         * Generate embeddings for the given input text(s) using the specified provider and model. This endpoint is compatible with OpenAI\'s embeddings API format, but includes an additional \'provider\' parameter to specify which LLM provider to use.  The endpoint supports both single text inputs and batch processing of multiple texts. It requires the provider to be configured in the user\'s Model Service Provider (MSP) settings with a valid API key. 
         * @summary Create embeddings
         * @param {LlmApiEmbeddingsPostRequest} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        embeddingsPost(requestParameters: LlmApiEmbeddingsPostRequest, options?: RawAxiosRequestConfig): AxiosPromise<EmbeddingResponse> {
            return localVarFp.embeddingsPost(requestParameters.embeddingRequest, options).then((request) => request(axios, basePath));
        },
        /**
         * Get complete LLM configuration including providers and models
         * @summary Get complete LLM configuration
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmConfigurationGet(options?: RawAxiosRequestConfig): AxiosPromise<LlmConfigurationResponse> {
            return localVarFp.llmConfigurationGet(options).then((request) => request(axios, basePath));
        },
        /**
         * List all LLM provider models, optionally filtered by provider
         * @summary List all LLM provider models
         * @param {LlmApiLlmProviderModelsGetRequest} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProviderModelsGet(requestParameters: LlmApiLlmProviderModelsGetRequest = {}, options?: RawAxiosRequestConfig): AxiosPromise<LlmProviderModelList> {
            return localVarFp.llmProviderModelsGet(requestParameters.providerName, options).then((request) => request(axios, basePath));
        },
        /**
         * Create a new LLM provider with optional API key
         * @summary Create a new LLM provider
         * @param {LlmApiLlmProvidersPostRequest} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersPost(requestParameters: LlmApiLlmProvidersPostRequest, options?: RawAxiosRequestConfig): AxiosPromise<LlmProvider> {
            return localVarFp.llmProvidersPost(requestParameters.llmProviderCreateWithApiKey, options).then((request) => request(axios, basePath));
        },
        /**
         * Delete an LLM provider (soft delete)
         * @summary Delete an LLM provider
         * @param {LlmApiLlmProvidersProviderNameDeleteRequest} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersProviderNameDelete(requestParameters: LlmApiLlmProvidersProviderNameDeleteRequest, options?: RawAxiosRequestConfig): AxiosPromise<void> {
            return localVarFp.llmProvidersProviderNameDelete(requestParameters.providerName, options).then((request) => request(axios, basePath));
        },
        /**
         * Get a specific LLM provider by name
         * @summary Get a specific LLM provider
         * @param {LlmApiLlmProvidersProviderNameGetRequest} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersProviderNameGet(requestParameters: LlmApiLlmProvidersProviderNameGetRequest, options?: RawAxiosRequestConfig): AxiosPromise<LlmProvider> {
            return localVarFp.llmProvidersProviderNameGet(requestParameters.providerName, options).then((request) => request(axios, basePath));
        },
        /**
         * Delete a specific model of a provider
         * @summary Delete a provider model
         * @param {LlmApiLlmProvidersProviderNameModelsApiModelDeleteRequest} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersProviderNameModelsApiModelDelete(requestParameters: LlmApiLlmProvidersProviderNameModelsApiModelDeleteRequest, options?: RawAxiosRequestConfig): AxiosPromise<void> {
            return localVarFp.llmProvidersProviderNameModelsApiModelDelete(requestParameters.providerName, requestParameters.api, requestParameters.model, options).then((request) => request(axios, basePath));
        },
        /**
         * Update a specific model of a provider
         * @summary Update a provider model
         * @param {LlmApiLlmProvidersProviderNameModelsApiModelPutRequest} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersProviderNameModelsApiModelPut(requestParameters: LlmApiLlmProvidersProviderNameModelsApiModelPutRequest, options?: RawAxiosRequestConfig): AxiosPromise<LlmProviderModel> {
            return localVarFp.llmProvidersProviderNameModelsApiModelPut(requestParameters.providerName, requestParameters.api, requestParameters.model, requestParameters.llmProviderModelUpdate, options).then((request) => request(axios, basePath));
        },
        /**
         * Get all models for a specific provider
         * @summary Get models for a specific provider
         * @param {LlmApiLlmProvidersProviderNameModelsGetRequest} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersProviderNameModelsGet(requestParameters: LlmApiLlmProvidersProviderNameModelsGetRequest, options?: RawAxiosRequestConfig): AxiosPromise<LlmProviderModelList> {
            return localVarFp.llmProvidersProviderNameModelsGet(requestParameters.providerName, options).then((request) => request(axios, basePath));
        },
        /**
         * Create a new model for a specific provider
         * @summary Create a new model for a provider
         * @param {LlmApiLlmProvidersProviderNameModelsPostRequest} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersProviderNameModelsPost(requestParameters: LlmApiLlmProvidersProviderNameModelsPostRequest, options?: RawAxiosRequestConfig): AxiosPromise<LlmProviderModel> {
            return localVarFp.llmProvidersProviderNameModelsPost(requestParameters.providerName, requestParameters.llmProviderModelCreate, options).then((request) => request(axios, basePath));
        },
        /**
         * Update an existing LLM provider with optional API key
         * @summary Update an LLM provider
         * @param {LlmApiLlmProvidersProviderNamePutRequest} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        llmProvidersProviderNamePut(requestParameters: LlmApiLlmProvidersProviderNamePutRequest, options?: RawAxiosRequestConfig): AxiosPromise<LlmProvider> {
            return localVarFp.llmProvidersProviderNamePut(requestParameters.providerName, requestParameters.llmProviderUpdateWithApiKey, options).then((request) => request(axios, basePath));
        },
        /**
         * Rerank a list of documents based on their relevance to a given query using the specified  provider and model. This endpoint follows the industry-standard rerank API format used by providers like Cohere, Jina AI, and others.  The endpoint supports both simple text lists and structured document objects with metadata. Documents are returned ordered by relevance score (highest first), with optional top_k  filtering to limit the number of results.  The provider must be configured in the user\'s Model Service Provider (MSP) settings with a valid API key and support rerank functionality. 
         * @summary Rerank documents
         * @param {LlmApiRerankPostRequest} requestParameters Request parameters.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        rerankPost(requestParameters: LlmApiRerankPostRequest, options?: RawAxiosRequestConfig): AxiosPromise<RerankResponse> {
            return localVarFp.rerankPost(requestParameters.rerankRequest, options).then((request) => request(axios, basePath));
        },
    };
};

/**
 * LlmApi - interface
 * @export
 * @interface LlmApi
 */
export interface LlmApiInterface {
    /**
     * Get available models with tag filtering support. - No tag_filters or empty array: Returns only models with \"recommend\" tag (default) - Specific filters: Returns models matching the filter conditions 
     * @summary Get available models with filtering
     * @param {LlmApiAvailableModelsPostRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApiInterface
     */
    availableModelsPost(requestParameters?: LlmApiAvailableModelsPostRequest, options?: RawAxiosRequestConfig): AxiosPromise<ModelConfigList>;

    /**
     * Generate embeddings for the given input text(s) using the specified provider and model. This endpoint is compatible with OpenAI\'s embeddings API format, but includes an additional \'provider\' parameter to specify which LLM provider to use.  The endpoint supports both single text inputs and batch processing of multiple texts. It requires the provider to be configured in the user\'s Model Service Provider (MSP) settings with a valid API key. 
     * @summary Create embeddings
     * @param {LlmApiEmbeddingsPostRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApiInterface
     */
    embeddingsPost(requestParameters: LlmApiEmbeddingsPostRequest, options?: RawAxiosRequestConfig): AxiosPromise<EmbeddingResponse>;

    /**
     * Get complete LLM configuration including providers and models
     * @summary Get complete LLM configuration
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApiInterface
     */
    llmConfigurationGet(options?: RawAxiosRequestConfig): AxiosPromise<LlmConfigurationResponse>;

    /**
     * List all LLM provider models, optionally filtered by provider
     * @summary List all LLM provider models
     * @param {LlmApiLlmProviderModelsGetRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApiInterface
     */
    llmProviderModelsGet(requestParameters?: LlmApiLlmProviderModelsGetRequest, options?: RawAxiosRequestConfig): AxiosPromise<LlmProviderModelList>;

    /**
     * Create a new LLM provider with optional API key
     * @summary Create a new LLM provider
     * @param {LlmApiLlmProvidersPostRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApiInterface
     */
    llmProvidersPost(requestParameters: LlmApiLlmProvidersPostRequest, options?: RawAxiosRequestConfig): AxiosPromise<LlmProvider>;

    /**
     * Delete an LLM provider (soft delete)
     * @summary Delete an LLM provider
     * @param {LlmApiLlmProvidersProviderNameDeleteRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApiInterface
     */
    llmProvidersProviderNameDelete(requestParameters: LlmApiLlmProvidersProviderNameDeleteRequest, options?: RawAxiosRequestConfig): AxiosPromise<void>;

    /**
     * Get a specific LLM provider by name
     * @summary Get a specific LLM provider
     * @param {LlmApiLlmProvidersProviderNameGetRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApiInterface
     */
    llmProvidersProviderNameGet(requestParameters: LlmApiLlmProvidersProviderNameGetRequest, options?: RawAxiosRequestConfig): AxiosPromise<LlmProvider>;

    /**
     * Delete a specific model of a provider
     * @summary Delete a provider model
     * @param {LlmApiLlmProvidersProviderNameModelsApiModelDeleteRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApiInterface
     */
    llmProvidersProviderNameModelsApiModelDelete(requestParameters: LlmApiLlmProvidersProviderNameModelsApiModelDeleteRequest, options?: RawAxiosRequestConfig): AxiosPromise<void>;

    /**
     * Update a specific model of a provider
     * @summary Update a provider model
     * @param {LlmApiLlmProvidersProviderNameModelsApiModelPutRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApiInterface
     */
    llmProvidersProviderNameModelsApiModelPut(requestParameters: LlmApiLlmProvidersProviderNameModelsApiModelPutRequest, options?: RawAxiosRequestConfig): AxiosPromise<LlmProviderModel>;

    /**
     * Get all models for a specific provider
     * @summary Get models for a specific provider
     * @param {LlmApiLlmProvidersProviderNameModelsGetRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApiInterface
     */
    llmProvidersProviderNameModelsGet(requestParameters: LlmApiLlmProvidersProviderNameModelsGetRequest, options?: RawAxiosRequestConfig): AxiosPromise<LlmProviderModelList>;

    /**
     * Create a new model for a specific provider
     * @summary Create a new model for a provider
     * @param {LlmApiLlmProvidersProviderNameModelsPostRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApiInterface
     */
    llmProvidersProviderNameModelsPost(requestParameters: LlmApiLlmProvidersProviderNameModelsPostRequest, options?: RawAxiosRequestConfig): AxiosPromise<LlmProviderModel>;

    /**
     * Update an existing LLM provider with optional API key
     * @summary Update an LLM provider
     * @param {LlmApiLlmProvidersProviderNamePutRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApiInterface
     */
    llmProvidersProviderNamePut(requestParameters: LlmApiLlmProvidersProviderNamePutRequest, options?: RawAxiosRequestConfig): AxiosPromise<LlmProvider>;

    /**
     * Rerank a list of documents based on their relevance to a given query using the specified  provider and model. This endpoint follows the industry-standard rerank API format used by providers like Cohere, Jina AI, and others.  The endpoint supports both simple text lists and structured document objects with metadata. Documents are returned ordered by relevance score (highest first), with optional top_k  filtering to limit the number of results.  The provider must be configured in the user\'s Model Service Provider (MSP) settings with a valid API key and support rerank functionality. 
     * @summary Rerank documents
     * @param {LlmApiRerankPostRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApiInterface
     */
    rerankPost(requestParameters: LlmApiRerankPostRequest, options?: RawAxiosRequestConfig): AxiosPromise<RerankResponse>;

}

/**
 * Request parameters for availableModelsPost operation in LlmApi.
 * @export
 * @interface LlmApiAvailableModelsPostRequest
 */
export interface LlmApiAvailableModelsPostRequest {
    /**
     * 
     * @type {TagFilterRequest}
     * @memberof LlmApiAvailableModelsPost
     */
    readonly tagFilterRequest?: TagFilterRequest
}

/**
 * Request parameters for embeddingsPost operation in LlmApi.
 * @export
 * @interface LlmApiEmbeddingsPostRequest
 */
export interface LlmApiEmbeddingsPostRequest {
    /**
     * 
     * @type {EmbeddingRequest}
     * @memberof LlmApiEmbeddingsPost
     */
    readonly embeddingRequest: EmbeddingRequest
}

/**
 * Request parameters for llmProviderModelsGet operation in LlmApi.
 * @export
 * @interface LlmApiLlmProviderModelsGetRequest
 */
export interface LlmApiLlmProviderModelsGetRequest {
    /**
     * Optional filter by provider name
     * @type {string}
     * @memberof LlmApiLlmProviderModelsGet
     */
    readonly providerName?: string
}

/**
 * Request parameters for llmProvidersPost operation in LlmApi.
 * @export
 * @interface LlmApiLlmProvidersPostRequest
 */
export interface LlmApiLlmProvidersPostRequest {
    /**
     * 
     * @type {LlmProviderCreateWithApiKey}
     * @memberof LlmApiLlmProvidersPost
     */
    readonly llmProviderCreateWithApiKey: LlmProviderCreateWithApiKey
}

/**
 * Request parameters for llmProvidersProviderNameDelete operation in LlmApi.
 * @export
 * @interface LlmApiLlmProvidersProviderNameDeleteRequest
 */
export interface LlmApiLlmProvidersProviderNameDeleteRequest {
    /**
     * Provider name
     * @type {string}
     * @memberof LlmApiLlmProvidersProviderNameDelete
     */
    readonly providerName: string
}

/**
 * Request parameters for llmProvidersProviderNameGet operation in LlmApi.
 * @export
 * @interface LlmApiLlmProvidersProviderNameGetRequest
 */
export interface LlmApiLlmProvidersProviderNameGetRequest {
    /**
     * Provider name
     * @type {string}
     * @memberof LlmApiLlmProvidersProviderNameGet
     */
    readonly providerName: string
}

/**
 * Request parameters for llmProvidersProviderNameModelsApiModelDelete operation in LlmApi.
 * @export
 * @interface LlmApiLlmProvidersProviderNameModelsApiModelDeleteRequest
 */
export interface LlmApiLlmProvidersProviderNameModelsApiModelDeleteRequest {
    /**
     * Provider name
     * @type {string}
     * @memberof LlmApiLlmProvidersProviderNameModelsApiModelDelete
     */
    readonly providerName: string

    /**
     * API type
     * @type {'completion' | 'embedding' | 'rerank'}
     * @memberof LlmApiLlmProvidersProviderNameModelsApiModelDelete
     */
    readonly api: LlmProvidersProviderNameModelsApiModelDeleteApiEnum

    /**
     * Model name (supports names with slashes, will be URL decoded)
     * @type {string}
     * @memberof LlmApiLlmProvidersProviderNameModelsApiModelDelete
     */
    readonly model: string
}

/**
 * Request parameters for llmProvidersProviderNameModelsApiModelPut operation in LlmApi.
 * @export
 * @interface LlmApiLlmProvidersProviderNameModelsApiModelPutRequest
 */
export interface LlmApiLlmProvidersProviderNameModelsApiModelPutRequest {
    /**
     * Provider name
     * @type {string}
     * @memberof LlmApiLlmProvidersProviderNameModelsApiModelPut
     */
    readonly providerName: string

    /**
     * API type
     * @type {'completion' | 'embedding' | 'rerank'}
     * @memberof LlmApiLlmProvidersProviderNameModelsApiModelPut
     */
    readonly api: LlmProvidersProviderNameModelsApiModelPutApiEnum

    /**
     * Model name (supports names with slashes, will be URL decoded)
     * @type {string}
     * @memberof LlmApiLlmProvidersProviderNameModelsApiModelPut
     */
    readonly model: string

    /**
     * 
     * @type {LlmProviderModelUpdate}
     * @memberof LlmApiLlmProvidersProviderNameModelsApiModelPut
     */
    readonly llmProviderModelUpdate: LlmProviderModelUpdate
}

/**
 * Request parameters for llmProvidersProviderNameModelsGet operation in LlmApi.
 * @export
 * @interface LlmApiLlmProvidersProviderNameModelsGetRequest
 */
export interface LlmApiLlmProvidersProviderNameModelsGetRequest {
    /**
     * Provider name
     * @type {string}
     * @memberof LlmApiLlmProvidersProviderNameModelsGet
     */
    readonly providerName: string
}

/**
 * Request parameters for llmProvidersProviderNameModelsPost operation in LlmApi.
 * @export
 * @interface LlmApiLlmProvidersProviderNameModelsPostRequest
 */
export interface LlmApiLlmProvidersProviderNameModelsPostRequest {
    /**
     * Provider name
     * @type {string}
     * @memberof LlmApiLlmProvidersProviderNameModelsPost
     */
    readonly providerName: string

    /**
     * 
     * @type {LlmProviderModelCreate}
     * @memberof LlmApiLlmProvidersProviderNameModelsPost
     */
    readonly llmProviderModelCreate: LlmProviderModelCreate
}

/**
 * Request parameters for llmProvidersProviderNamePut operation in LlmApi.
 * @export
 * @interface LlmApiLlmProvidersProviderNamePutRequest
 */
export interface LlmApiLlmProvidersProviderNamePutRequest {
    /**
     * Provider name
     * @type {string}
     * @memberof LlmApiLlmProvidersProviderNamePut
     */
    readonly providerName: string

    /**
     * 
     * @type {LlmProviderUpdateWithApiKey}
     * @memberof LlmApiLlmProvidersProviderNamePut
     */
    readonly llmProviderUpdateWithApiKey: LlmProviderUpdateWithApiKey
}

/**
 * Request parameters for rerankPost operation in LlmApi.
 * @export
 * @interface LlmApiRerankPostRequest
 */
export interface LlmApiRerankPostRequest {
    /**
     * 
     * @type {RerankRequest}
     * @memberof LlmApiRerankPost
     */
    readonly rerankRequest: RerankRequest
}

/**
 * LlmApi - object-oriented interface
 * @export
 * @class LlmApi
 * @extends {BaseAPI}
 */
export class LlmApi extends BaseAPI implements LlmApiInterface {
    /**
     * Get available models with tag filtering support. - No tag_filters or empty array: Returns only models with \"recommend\" tag (default) - Specific filters: Returns models matching the filter conditions 
     * @summary Get available models with filtering
     * @param {LlmApiAvailableModelsPostRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApi
     */
    public availableModelsPost(requestParameters: LlmApiAvailableModelsPostRequest = {}, options?: RawAxiosRequestConfig) {
        return LlmApiFp(this.configuration).availableModelsPost(requestParameters.tagFilterRequest, options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * Generate embeddings for the given input text(s) using the specified provider and model. This endpoint is compatible with OpenAI\'s embeddings API format, but includes an additional \'provider\' parameter to specify which LLM provider to use.  The endpoint supports both single text inputs and batch processing of multiple texts. It requires the provider to be configured in the user\'s Model Service Provider (MSP) settings with a valid API key. 
     * @summary Create embeddings
     * @param {LlmApiEmbeddingsPostRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApi
     */
    public embeddingsPost(requestParameters: LlmApiEmbeddingsPostRequest, options?: RawAxiosRequestConfig) {
        return LlmApiFp(this.configuration).embeddingsPost(requestParameters.embeddingRequest, options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * Get complete LLM configuration including providers and models
     * @summary Get complete LLM configuration
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApi
     */
    public llmConfigurationGet(options?: RawAxiosRequestConfig) {
        return LlmApiFp(this.configuration).llmConfigurationGet(options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * List all LLM provider models, optionally filtered by provider
     * @summary List all LLM provider models
     * @param {LlmApiLlmProviderModelsGetRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApi
     */
    public llmProviderModelsGet(requestParameters: LlmApiLlmProviderModelsGetRequest = {}, options?: RawAxiosRequestConfig) {
        return LlmApiFp(this.configuration).llmProviderModelsGet(requestParameters.providerName, options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * Create a new LLM provider with optional API key
     * @summary Create a new LLM provider
     * @param {LlmApiLlmProvidersPostRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApi
     */
    public llmProvidersPost(requestParameters: LlmApiLlmProvidersPostRequest, options?: RawAxiosRequestConfig) {
        return LlmApiFp(this.configuration).llmProvidersPost(requestParameters.llmProviderCreateWithApiKey, options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * Delete an LLM provider (soft delete)
     * @summary Delete an LLM provider
     * @param {LlmApiLlmProvidersProviderNameDeleteRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApi
     */
    public llmProvidersProviderNameDelete(requestParameters: LlmApiLlmProvidersProviderNameDeleteRequest, options?: RawAxiosRequestConfig) {
        return LlmApiFp(this.configuration).llmProvidersProviderNameDelete(requestParameters.providerName, options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * Get a specific LLM provider by name
     * @summary Get a specific LLM provider
     * @param {LlmApiLlmProvidersProviderNameGetRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApi
     */
    public llmProvidersProviderNameGet(requestParameters: LlmApiLlmProvidersProviderNameGetRequest, options?: RawAxiosRequestConfig) {
        return LlmApiFp(this.configuration).llmProvidersProviderNameGet(requestParameters.providerName, options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * Delete a specific model of a provider
     * @summary Delete a provider model
     * @param {LlmApiLlmProvidersProviderNameModelsApiModelDeleteRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApi
     */
    public llmProvidersProviderNameModelsApiModelDelete(requestParameters: LlmApiLlmProvidersProviderNameModelsApiModelDeleteRequest, options?: RawAxiosRequestConfig) {
        return LlmApiFp(this.configuration).llmProvidersProviderNameModelsApiModelDelete(requestParameters.providerName, requestParameters.api, requestParameters.model, options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * Update a specific model of a provider
     * @summary Update a provider model
     * @param {LlmApiLlmProvidersProviderNameModelsApiModelPutRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApi
     */
    public llmProvidersProviderNameModelsApiModelPut(requestParameters: LlmApiLlmProvidersProviderNameModelsApiModelPutRequest, options?: RawAxiosRequestConfig) {
        return LlmApiFp(this.configuration).llmProvidersProviderNameModelsApiModelPut(requestParameters.providerName, requestParameters.api, requestParameters.model, requestParameters.llmProviderModelUpdate, options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * Get all models for a specific provider
     * @summary Get models for a specific provider
     * @param {LlmApiLlmProvidersProviderNameModelsGetRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApi
     */
    public llmProvidersProviderNameModelsGet(requestParameters: LlmApiLlmProvidersProviderNameModelsGetRequest, options?: RawAxiosRequestConfig) {
        return LlmApiFp(this.configuration).llmProvidersProviderNameModelsGet(requestParameters.providerName, options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * Create a new model for a specific provider
     * @summary Create a new model for a provider
     * @param {LlmApiLlmProvidersProviderNameModelsPostRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApi
     */
    public llmProvidersProviderNameModelsPost(requestParameters: LlmApiLlmProvidersProviderNameModelsPostRequest, options?: RawAxiosRequestConfig) {
        return LlmApiFp(this.configuration).llmProvidersProviderNameModelsPost(requestParameters.providerName, requestParameters.llmProviderModelCreate, options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * Update an existing LLM provider with optional API key
     * @summary Update an LLM provider
     * @param {LlmApiLlmProvidersProviderNamePutRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApi
     */
    public llmProvidersProviderNamePut(requestParameters: LlmApiLlmProvidersProviderNamePutRequest, options?: RawAxiosRequestConfig) {
        return LlmApiFp(this.configuration).llmProvidersProviderNamePut(requestParameters.providerName, requestParameters.llmProviderUpdateWithApiKey, options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * Rerank a list of documents based on their relevance to a given query using the specified  provider and model. This endpoint follows the industry-standard rerank API format used by providers like Cohere, Jina AI, and others.  The endpoint supports both simple text lists and structured document objects with metadata. Documents are returned ordered by relevance score (highest first), with optional top_k  filtering to limit the number of results.  The provider must be configured in the user\'s Model Service Provider (MSP) settings with a valid API key and support rerank functionality. 
     * @summary Rerank documents
     * @param {LlmApiRerankPostRequest} requestParameters Request parameters.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof LlmApi
     */
    public rerankPost(requestParameters: LlmApiRerankPostRequest, options?: RawAxiosRequestConfig) {
        return LlmApiFp(this.configuration).rerankPost(requestParameters.rerankRequest, options).then((request) => request(this.axios, this.basePath));
    }
}

/**
 * @export
 */
export const LlmProvidersProviderNameModelsApiModelDeleteApiEnum = {
    completion: 'completion',
    embedding: 'embedding',
    rerank: 'rerank'
} as const;
export type LlmProvidersProviderNameModelsApiModelDeleteApiEnum = typeof LlmProvidersProviderNameModelsApiModelDeleteApiEnum[keyof typeof LlmProvidersProviderNameModelsApiModelDeleteApiEnum];
/**
 * @export
 */
export const LlmProvidersProviderNameModelsApiModelPutApiEnum = {
    completion: 'completion',
    embedding: 'embedding',
    rerank: 'rerank'
} as const;
export type LlmProvidersProviderNameModelsApiModelPutApiEnum = typeof LlmProvidersProviderNameModelsApiModelPutApiEnum[keyof typeof LlmProvidersProviderNameModelsApiModelPutApiEnum];
