LLM_MODEL=vicuna-13b
MODEL_FAMILIES=[{"name":"qianwen","label":"QianWen","enabled":"true","temperature":0.01,"models":[{"name":"qwen-turbo","label":"QianWenTurbo","enabled":"true","memory":"enabled","context_window":8096},{"name":"qwen-plus","label":"QianWenPlus","enabled":"true","memory":"enabled","context_window":8096},{"name":"qwen-max","label":"QianWenMax","enabled":"true","memory":"enabled","context_window":8096}]},{"name":"azure-openai","label":"AzureOpenAI","enabled":"true","temperature":0,"models":[{"name":"azure-openai","label":"AzureOpenAI","enabled":"true","memory":"enabled","context_window":4096,"free_tier":true}]},{"name":"chatgpt","label":"ChatGPT","enabled":"true","temperature":0,"models":[{"name":"gpt-4","label":"ChatGPT-4","enabled":"true","memory":"enabled","context_window":8192}]},{"name":"deepseek","label":"DeepSeek","enabled":"true","models":[{"name":"deepseek-chat","label":"DeepSeekChat","enabled":"true","memory":"disabled","free_tier":true,"context_window":128000,"similarity_topk":10}]},{"name":"glm-4","label":"GLM-4","enabled":"true","models":[{"name":"glm-4-plus","label":"GLM-4-Plus","enabled":"true","memory":"enabled","context_window":128000,"similarity_topk":10,"endpoint":"https://open.bigmodel.cn/api/paas/v4/"},{"name":"glm-4-air","label":"GLM-4-Air","enabled":"true","memory":"enabled","context_window":128000,"similarity_topk":10,"endpoint":"https://open.bigmodel.cn/api/paas/v4/"},{"name":"glm-4-long","label":"GLM-4-Long","enabled":"true","memory":"enabled","context_window":1000000,"similarity_topk":20,"endpoint":"https://open.bigmodel.cn/api/paas/v4/"},{"name":"glm-4-flashx","label":"GLM-4-FlashX","enabled":"true","memory":"enabled","context_window":128000,"similarity_topk":10,"endpoint":"https://open.bigmodel.cn/api/paas/v4/"},{"name":"glm-4-flash","label":"GLM-4-Flash","enabled":"true","memory":"enabled","context_window":128000,"similarity_topk":10,"endpoint":"https://open.bigmodel.cn/api/paas/v4/","free_tier":true}]}]
LIMIT_MODEL_CONCURRENCY=5
MAX_POSITION_EMBEDDINGS=4096
QUANTIZE_QLORA=True

# MetaDB
POSTGRES_HOST=127.0.0.1
POSTGRES_PORT=5432
POSTGRES_DB=aperag
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres



# Auth
AUTH_TYPE=none
AUTH0_DOMAIN=
AUTH0_CLIENT_ID=
AUTHING_DOMAIN=
AUTHING_APP_ID=
LOGTO_DOMAIN=
LOGTO_APP_ID=

# Logging
DJANGO_LOG_LEVEL=INFO


# Celery
CELERY_BROKER_URL=redis://default:password@127.0.0.1:6379/0
CELERY_FLOWER_USER=admin
CELERY_FLOWER_PASSWORD=admin

# MetaDB
DATABASE_URL="postgres://postgres:postgres@127.0.0.1:5432/postgres"

# Vector DB
VECTOR_DB_TYPE=qdrant
VECTOR_DB_CONTEXT={"url":"http://127.0.0.1", "port":6333, "distance":"Cosine", "timeout": 1000}

# Elasticsearch
ES_HOST=http://127.0.0.1:9200

# Redis
MEMORY_REDIS_URL=redis://default:password@127.0.0.1:6379


DEBUG=False
FEISHU_APP_ID=
FEISHU_APP_SECRET=
#FEISHU_ENCRYPT_KEY=

MAX_BOT_COUNT=10
MAX_COLLECTION_COUNT=50
MAX_DOCUMENT_COUNT=1000
MAX_CONVERSATION_COUNT=100

EMBEDDING_MODEL=bge
EMBEDDING_DEVICE=cpu
EMBEDDING_BACKEND=local
EMBEDDING_SERVICE_URL=http://localhost:9997
EMBEDDING_SERVICE_MODEL=bge-large-zh-v1.5
EMBEDDING_SERVICE_TOKEN=
EMBEDDING_SERVICE_MODEL_UID=


CHAT_CONSUMER_IMPLEMENTATION=document-qa


RERANK_MODEL_PATH=~/.cache/huggingface/hub/bge-reranker-large
RERANK_BACKEND=local
RERANK_SERVICE_URL=http://localhost:9997
RERANK_SERVICE_MODEL_UID=

# Specifies the cache directory for tiktoken. This prevents the need to re-download models on each run.
# You can also pre-download models to this directory.
#
# For example:
#   TIKTOKEN_URL="https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken"
#   wget -O "${TIKTOKEN_CACHE_DIR}/$(echo -n "$TIKTOKEN_URL" | sha1sum | head -c 40)" "$TIKTOKEN_URL"
TIKTOKEN_CACHE_DIR=.cache/tiktoken
DEFAULT_ENCODING_MODEL=cl100k_base

# Generated by scripts/download_mineru_models.py
MINERU_CONFIG_JSON=./magic-pdf.json
MINERU_TEMP_FILE_DIR=

#OPENAI_API_PROXY='{"https": "socks5h://127.0.0.1:1080"}'
OPENAI_API_BASE=
OPENAI_API_KEY=
