LLM_MODEL=vicuna-13b
MODEL_FAMILIES=[{"name":"qianwen","label":"QianWen","enabled":"true","temperature":0.01,"models":[{"name":"qwen-turbo","label":"QianWen Turbo","enabled":"true","memory":"enabled"},{"name":"qwen-plus","label":"QianWen Plus","enabled":"true","memory":"enabled"},{"name":"qwen-max","label":"QianWen Max","enabled":"true","memory":"enabled"}]},{"name":"chatglm","label":"ChatGLM","enabled":"true","temperature":0.01,"models":[{"name":"chatglm-turbo","label":"ChatGLM Turbo","enabled":"true","memory":"enabled"},{"name":"chatglm-std","label":"ChatGLM Std","enabled":"true","memory":"enabled"},{"name":"chatglm-lite","label":"ChatGLM Lite","enabled":"true","memory":"enabled"},{"name":"chatglm-pro","label":"ChatGLM Pro","enabled":"true","memory":"enabled"},{"name":"chatglm2-6b","label":"ChatGLM2 6b","endpoint":"http://llmserver-chatglm2-6b:8000","enabled":"false","memory":"disabled"}]},{"name":"baichuan","label":"BaiChuan","enabled":"true","temperature":0.01,"models":[{"name":"baichuan-13b","label":"BaiChuan 13b","enabled":"false","memory":"disabled","endpoint":"http://llmserver-baichuan-13b:8000"},{"name":"baichuan-53b","label":"BaiChuan 53b","enabled":"true","memory":"disabled"}]},{"name":"azure-openai","label":"Azure OpenAI","enabled":"true","temperature":0,"models":[{"name":"azure-openai","label":"Azure OpenAI","enabled":"true","memory":"enabled"}]},{"name":"chatgpt","label":"ChatGPT","enabled":"true","temperature":0,"models":[{"name":"gpt-4-1106-preview","label":"ChatGPT-4 Turbo","enabled":"true","memory":"enabled","context_window":128000,"similarity_topk":10},{"name":"gpt-4","label":"ChatGPT-4","enabled":"true","memory":"enabled","context_window":8192},{"name":"gpt-4-0613","label":"ChatGPT-4-0613","enabled":"true","memory":"enabled","context_window":8192},{"name":"gpt-3.5-turbo-1106","label":"ChatGPT 3.5 Turbo 1106","enabled":"true","memory":"enabled","context_window":16385,"similarity_topk":5},{"name":"gpt-3.5-turbo","label":"ChatGPT-3.5 Turbo","enabled":"true","memory":"enabled","context_window":4096},{"name":"gpt-3.5-turbo-16k","label":"ChatGPT-3.5 Turbo 16k","enabled":"true","memory":"enabled","context_window":16384,"similarity_topk":5}]},{"name":"wenxinyiyan","label":"Wen Xin Yi Yan","enabled":"true","models":[{"name":"ernie-bot-turbo","label":"Wen Xin Yi Yan","enabled":"true","memory":"disabled"}]},{"name":"vicuna","label":"Vicuna","enabled":"false","models":[{"name":"vicuna-13b","label":"Vicuna 13b","enabled":"false","memory":"disabled","endpoint":"http://llmserver-vicuna-13b:8000"}]},{"name":"guanaco","label":"Guanaco","enabled":"false","models":[{"name":"guanaco-33b","label":"Guanaco 33b","enabled":"false","memory":"disabled","endpoint":"http://llmserver-guanaco-33b:8000"}]},{"name":"falcon","label":"Falcon","enabled":"false","models":[{"name":"falcon-40b","label":"Falcon 40b","enabled":"false","memory":"disabled","endpoint":"http://llmserver-falcon-40b:8000"}]},{"name":"gorilla","label":"Gorilla","enabled":"false","models":[{"name":"gorilla-7b","label":"Gorilla 7b","enabled":"false","memory":"disabled","endpoint":"http://llmserver-gurilla-7b:8000"}]}]
LIMIT_MODEL_CONCURRENCY=5
MAX_POSITION_EMBEDDINGS=4096
QUANTIZE_QLORA=True

# MetaDB
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=deeprag
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres



# Auth
AUTH_TYPE=none
AUTH0_DOMAIN=
AUTH0_CLIENT_ID=
AUTHING_DOMAIN=
AUTHING_APP_ID=
LOGTO_DOMAIN=
LOGTO_APP_ID=

# Redis
MEMORY_REDIS_URL=redis://default:password@127.0.0.1:6379


# Logging
DJANGO_LOG_LEVEL=INFO


# Celery
CELERY_BROKER_URL=redis://default:password@127.0.0.1:6379/0
CELERY_FLOWER_USER=admin
CELERY_FLOWER_PASSWORD=admin

# Vector DB
VECTOR_DB_TYPE=qdrant
VECTOR_DB_CONTEXT={"url":"http://127.0.0.1", "port":6333, "distance":"Cosine", "timeout": 1000}


DEBUG=False
FEISHU_APP_ID=
FEISHU_APP_SECRET=
#FEISHU_ENCRYPT_KEY=

MAX_BOT_COUNT=10
MAX_COLLECTION_COUNT=50
MAX_DOCUMENT_COUNT=1000
MAX_CONVERSATION_COUNT=100

EMBEDDING_MODEL=bge
EMBEDDING_DEVICE=cpu
EMBEDDING_BACKEND=local
EMBEDDING_SERVICE_URL=http://localhost:9997
EMBEDDING_SERVICE_MODEL=bge-large-zh-v1.5
EMBEDDING_SERVICE_TOKEN=
EMBEDDING_SERVICE_MODEL_UID=
OPENAI_API_KEY=


CHAT_CONSUMER_IMPLEMENTATION=document-qa


RERANK_MODEL_PATH=~/.cache/huggingface/hub/bge-reranker-large
RERANK_BACKEND=local
RERANK_SERVICE_URL=http://localhost:9997
RERANK_SERVICE_MODEL_UID=

# Specifies the cache directory for tiktoken. This prevents the need to re-download models on each run.
# You can also pre-download models to this directory.
#
# For example:
#   TIKTOKEN_URL="https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken"
#   wget -O "${TIKTOKEN_CACHE_DIR}/$(echo -n "$TIKTOKEN_URL" | sha1sum | head -c 40)" "$TIKTOKEN_URL"
TIKTOKEN_CACHE_DIR=~/.cache/tiktoken
DEFAULT_ENCODING_MODEL=cl100k_base


DATABASE_URL="postgres://postgres:postgres@127.0.0.1:5432/postgres"

#OPENAI_API_PROXY='{"https": "socks5h://127.0.0.1:1080"}'
